# AI 驱动的智能新闻聚合器

本项目是一个自动化新闻处理工具，能从多个网站并发抓取最新新闻，利用AI进行智能分类和排序，并根据您的个人配置，每日生成一份精简、高质量的新闻摘要。

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## 🚀 快速上手指南

只需四步，即可运行您的个人新闻助手。

### 1. 下载项目

首先，将项目代码下载到您的本地电脑。

```bash
git clone https://github.com/your-username/your-repository-name.git
cd your-repository-name
```

### 2. 安装依赖

本项目需要 Python 3.9 或更高版本。建议在虚拟环境中安装。

```bash
# 进入项目目录
# (可选，但推荐) 创建并激活虚拟环境
# python -m venv venv
# .\venv\Scripts\activate  (Windows)
# source venv/bin/activate (macOS/Linux)

# 安装所有必要的库
pip install -r requirements.txt
```
*(`requirements.txt` 文件已包含在项目中)*

### 3. 配置您的助手

- **设置API密钥**:
  1.  使用环境变量配置您自己的 **Google Gemini API 密钥**，`export GOOGLE_API_KEY="your-api-key"`

- **定制新闻内容**:
  1.  打开 `config.yaml` 文件。
  2.  **修改 `sites` 列表**: 填入您自己想看的新闻网站的RSS地址。
  3.  **修改 `force_keep_rules`**: 定义您想要强制保留的关键词或来源。
  4.  **修改 `category_quotas`**: 调整不同新闻类别（如“科技与商业”、“国际动态”）的最终数量配比。
  5.  **(可选) 配置代理**: 如果您需要通过代理访问网络，请在 `proxy` 部分填入您的代理地址，默认为127.0.0.1:7890。

### 4. 运行！

一切准备就绪，运行主程序即可。

```bash
python main.py
```
程序运行完毕后，您就可以在项目文件夹中找到生成的 `新闻精简_YYYYMMDD.txt` 文件以及 `FinalList_...` 文件夹中的详细数据了。

---

## 🔧 项目说明
### 核心功能详解

- **并发与增量采集**:
  - **并发处理**: 使用线程池 (`ThreadPoolExecutor`) 并发抓取多个RSS源，极大缩短了数据采集阶段的耗时。
  - **增量更新**: 通过与SQLite数据库比对URL，系统每次运行时只处理**真正新增**的文章，避免了对海量历史数据的重复分析。

- **数据库持久化**:
  - **SQLite集成**: 所有采集到的文章元数据都被持久化存储在SQLite数据库中，保证了数据的安全和可追溯性。
  - **自动数据管理**: 内置了老旧数据自动清理机制，可根据配置（如保留7天）定期删除过期数据，防止数据库无限膨胀。

- **AI 智能分析与评分**:
  - **动态分类**: 利用Google Gemini API，自动将每条新闻的标题精准地归类到“科技与商业”、“国际动态”、“社会与文化”、“国内政策”等预设类别中。
  - **多维重要性评分**: 通过精心设计的Prompt，引导AI从行业影响力、公众关注度、稀缺性与**反常性**等多个维度为新闻打分，能有效识别“出圈”热点。

- **高度可定制的筛选引擎**:
  - **配置驱动**: 所有的核心逻辑参数，如新闻源、强制保留规则、类别配额等，均由外部 `config.yaml` 文件定义，实现了代码与配置的完全分离。
  - **灵活的强制保留规则**: 用户可通过组合`关键词`、`来源 (source)`、`类别 (category)`等条件，轻松定义必须保留的新闻。
  - **智能配额筛选**: 支持为不同类别设置入选数量上限，确保最终输出的新闻摘要在内容领域上保持均衡和多样性。

- **健壮的工程实践**:
  - **模块化设计**: 项目被清晰地划分为数据解析(`parsers`)、AI处理(`ai_processing`)、数据库管理(`database`)、日志(`logger_config`)和主流程编排(`main`)等模块。
  - **日志系统**: 集成了Python的`logging`模块，将详细的运行状态和错误信息输出到带时间戳的日志文件中，实现了生产级的可观测性。

### 工作流程

```
            +---------------------------+
            |    1. 从SQLite读取已有URL   |
            +-------------+-------------+
                          |
                          v
            +-------------+-------------+
            |  2. 并发拉取RSS源 & 解析  |
            +-------------+-------------+
                          | (与已有URL比对)
                          v
            +-------------+-------------+
            |   3. 筛选出今日新增文章    |
            +-------------+-------------+
                          |
                          v
            +-------------+-------------+
            | 4. AI 动态分类与重要性评分 |
            |  (仅针对新增文章)         |
            +-------------+-------------+
                          |
                          v
            +-------------+-------------+
            |  5. 规则引擎筛选与配额处理  |
            +-------------+-------------+
                          |
                          v
            +-------------+-------------+
            |  6. AI 内容精简 & 输出报告  |
            +-------------+-------------+
                          |
                          v
            +-------------+-------------+
            | 7. 将新文章写入SQLite & 清理旧数据 |
            +---------------------------+
```

### 技术栈

- **核心语言**: Python 3.9+
- **数据处理**: `Pandas`
- **数据库**: `SQLite3`
- **网络与解析**: `feedparser`, `requests`, `BeautifulSoup4`, `newspaper3k`
- **AI 模型**: Google Gemini API (`google-generativeai`)
- **并发处理**: `concurrent.futures.ThreadPoolExecutor`
- **配置与环境**: `PyYAML`, `python-dotenv`
- **日志**: `logging`

### 未来路线
-   [ ] **反馈与模型微调**: 建立一个简单的反馈机制，收集用户偏好以实现个性化推荐。
-   [ ] **实现数据分析可视化**：绘制新闻来源数量条形图，生成热门关键词的词云
---

## 🤝 贡献与反馈

如果您在使用过程中遇到任何问题，或有任何改进建议，欢迎随时提交 [Issues](https://github.com/your-username/your-repository-name/issues)！

## 📄 许可证

本项目采用 [MIT License](LICENSE)。
